import json
import subprocess
import shutil
import time
import httpx
from pathlib import Path
from typing import List, Dict
from openai import OpenAI
from .config import settings


class MatchProcessor:
    def __init__(self, input_file: Path):
        self.input_file = input_file
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É reports/–∏–º—è_—Ñ–∞–π–ª–∞
        self.output_dir = Path(settings.OUTPUT_DIR) / input_file.stem
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # –ë–µ—Ä–µ–º –¥–ª–∏–Ω—É –∫—É—Å–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–ø–æ –¥–µ—Ñ–æ–ª—Ç—É 300 —Å–µ–∫)
        self.chunk_len = settings.CHUNK_LENGTH

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ .env)
        http_client = None
        if settings.PROXY_URL:
            print(f"üåç Using Proxy: {settings.PROXY_URL}")
            http_client = httpx.Client(
                proxy=settings.PROXY_URL,
                timeout=120.0  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö —Ñ–∞–π–ª–æ–≤
            )

            self.audio_client = OpenAI(
                base_url="https://api.groq.com/openai/v1",
                api_key=settings.API_KEY,
                http_client=http_client
            )

            llm_base = settings.LLM_API_BASE_URL or settings.API_BASE_URL
            llm_key = settings.LLM_API_KEY or settings.API_KEY

            self.llm_client = OpenAI(
                base_url=llm_base,
                api_key=llm_key,
                http_client=http_client
            )
            if settings.LLM_API_BASE_URL:
                print(f"üß† LLM Client connected to: {settings.LLM_API_BASE_URL}")

    def split_audio(self) -> List[Path]:
        """–†–µ–∂–µ—Ç –∏ —Å–∂–∏–º–∞–µ—Ç –∞—É–¥–∏–æ, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç—ã API (25MB)"""
        print(f"üî™ –ù–∞—Ä–µ–∑–∞–µ–º {self.input_file.name}...")
        segment_pattern = self.output_dir / "part_%03d.mp3"

        cmd = [
            "ffmpeg", "-y",
            "-i", str(self.input_file),
            "-f", "segment",
            "-segment_time", str(self.chunk_len),
            # –°–∂–∏–º–∞–µ–º –≤ –º–æ–Ω–æ 16–∫–ì—Ü –¥–ª—è –ª–µ–≥–∫–æ—Å—Ç–∏ (Whisper —ç—Ç–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
            "-ac", "1", "-ar", "16000", "-c:a", "libmp3lame", "-b:a", "48k",
            "-vn", "-loglevel", "error",
            str(segment_pattern)
        ]
        subprocess.run(cmd, check=True)
        return sorted(self.output_dir.glob("part_*.mp3"))

    def transcribe_chunk(self, file_path: Path):
        """–ó–∞–ø—Ä–æ—Å –∫ Whisper —Å —É–º–Ω—ã–º –æ–∂–∏–¥–∞–Ω–∏–µ–º –ª–∏–º–∏—Ç–æ–≤"""
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–æ 5
        max_retries = 5

        for attempt in range(max_retries):
            try:
                with open(file_path, "rb") as f:
                    return self.audio_client.audio.transcriptions.create(
                        file=f,
                        model=settings.WHISPER_MODEL,
                        language="ru",
                        response_format="verbose_json"
                    )
            except Exception as e:
                error_str = str(e)

                if "429" in error_str or "Rate limit" in error_str:
                    wait_time = 60 * (attempt + 1)
                    print(f"‚ö†Ô∏è Rate Limit (Groq 429). –ñ–¥–µ–º {wait_time} —Å–µ–∫... (–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
                    time.sleep(wait_time)

                elif "Connection error" in error_str:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–µ—Ç–∏. –ñ–¥–µ–º 10 —Å–µ–∫... (–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
                    time.sleep(10)

                else:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏: {e}")
                    time.sleep(5)

        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å {file_path.name} –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫.")
        return None

    def analyze_text(self, text_with_timestamps: str) -> Dict:
        """–¢–≤–æ–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        system_instruction = """
        –¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π –æ–±–æ–∑—Ä–µ–≤–∞—Ç–µ–ª—å.
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ–∑–¥–∞—Ç—å —Ö—Ä–æ–Ω–∏–∫—É –º–∞—Ç—á–∞, –≤—ã–¥–µ–ª–∏–≤ –í–°–ï –∑–Ω–∞—á–∏–º—ã–µ —ç–ø–∏–∑–æ–¥—ã.
        –¢–≤–æ–π –≥–ª–∞–≤–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –æ—Ç–±–æ—Ä–∞: "–î–æ—Å—Ç–æ–π–Ω–æ –ª–∏ —ç—Ç–æ –ø–æ–ø–∞—Å—Ç—å –≤ –≤–∏–¥–µ–æ-–æ–±–∑–æ—Ä –º–∞—Ç—á–∞?"

        –ö–†–ò–¢–ï–†–ò–ò –ó–ù–ê–ß–ò–ú–û–°–¢–ò:
        1. –í–ª–∏—è–Ω–∏–µ –Ω–∞ —Å—á–µ—Ç (–ì–æ–ª—ã, –æ—Ç–º–µ–Ω–µ–Ω–Ω—ã–µ –≥–æ–ª—ã).
        2. –û—Å—Ç—Ä–æ—Ç–∞ (–£–¥–∞—Ä—ã, —Å–µ–π–≤—ã –≤—Ä–∞—Ç–∞—Ä–µ–π, —à—Ç–∞–Ω–≥–∏, –ø–µ—Ä–µ–∫–ª–∞–¥–∏–Ω—ã, –æ–ø–∞—Å–Ω—ã–µ –ø—Ä–æ—Å—Ç—Ä–µ–ª—ã).
        3. –î–∏—Å—Ü–∏–ø–ª–∏–Ω–∞ –∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã (–ö–∞—Ä—Ç–æ—á–∫–∏, –ø–æ—Ç–∞—Å–æ–≤–∫–∏, —Å–ø–æ—Ä—ã —Å —Å—É–¥—å–µ–π, –≥—Ä—É–±—ã–µ —Ñ–æ–ª—ã).
        4. –ò–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∏–≥—Ä–µ (–ó–∞–º–µ–Ω—ã, —Ç—Ä–∞–≤–º—ã).
        5. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã (–ö—Ä–∞—Å–∏–≤—ã–µ —Ñ–∏–Ω—Ç—ã, –æ—à–∏–±–∫–∏ –∑–∞—â–∏—Ç—ã, —Ä–µ–∞–∫—Ü–∏—è —Ç—Ä–∏–±—É–Ω).

        –ò–ì–ù–û–†–ò–†–û–í–ê–¢–¨:
        1. –†–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤—ã ("–ê –ø–æ–º–Ω–∏—Ç–µ 2010 –≥–æ–¥...").
        2. –ü–æ–≤—Ç–æ—Ä—ã –æ–±—Å—É–∂–¥–µ–Ω–∏–π.
        3. –†—É—Ç–∏–Ω—É (–ø–∞—Å —Ä–∞–¥–∏ –ø–∞—Å–∞, –∞—É—Ç—ã).

        –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –û–¢–í–ï–ß–ê–ô –í –§–û–†–ú–ê–¢–ï JSON:
        {
          "events": [
            {
              "time": "–ú–ú:–°–°",
              "type": "–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–≥ (–ì–û–õ, –°–ï–ô–í, –§–û–õ, –ê–¢–ê–ö–ê)",
              "description": "–ñ–∏–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≥–æ, —á—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ"
            }
          ]
        }
        """

        try:
            response = self.llm_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_instruction},
                    {"role": "user", "content": f"–§—Ä–∞–≥–º–µ–Ω—Ç –º–∞—Ç—á–∞:\n{text_with_timestamps}"}
                ],
                model=settings.LLM_MODEL,
                response_format={"type": "json_object"},
                temperature=0.1,
            )

            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç Markdown (–≤–∞–∂–Ω–æ –¥–ª—è Llama 3)
            content = response.choices[0].message.content
            if content.startswith("```"):
                content = content.replace("```json", "").replace("```", "").strip()

            return json.loads(content)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ LLM –∏–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            return {"events": []}

    def run(self):
        # 1. –ù–∞—Ä–µ–∑–∫–∞
        chunks = self.split_audio()
        full_report = []

        print(f"üöÄ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(chunks)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")

        for i, chunk_path in enumerate(chunks):
            # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤
            if chunk_path.stat().st_size < 1000:
                continue

            # 2. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
            result = self.transcribe_chunk(chunk_path)
            if not result:
                continue

            # 3. –°–±–æ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ê–ë–°–û–õ–Æ–¢–ù–´–ú –≤—Ä–µ–º–µ–Ω–µ–º
            chunk_offset_seconds = i * self.chunk_len
            formatted_lines = []

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            if isinstance(result, dict):
                segments = result.get('segments', [])
            else:
                segments = getattr(result, 'segments', [])

            for seg in segments:
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ start –∏ text
                if isinstance(seg, dict):
                    start = seg.get('start', 0)
                    text = seg.get('text', '')
                else:
                    start = getattr(seg, 'start', 0)
                    text = getattr(seg, 'text', '')

                # –°—á–∏—Ç–∞–µ–º –≤—Ä–µ–º—è
                abs_time = chunk_offset_seconds + start
                mm = int(abs_time // 60)
                ss = int(abs_time % 60)

                formatted_lines.append(f"[{mm:02d}:{ss:02d}] {text}")

            full_text_chunk = "\n".join(formatted_lines)

            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –º–∞–ª–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if len(full_text_chunk) < 50:
                continue

            # 4. –ê–Ω–∞–ª–∏–∑ LLM
            print(f"üß† –ê–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ {i + 1}/{len(chunks)}...")
            analysis = self.analyze_text(full_text_chunk)

            if analysis.get("events"):
                for evt in analysis["events"]:
                    t = evt.get('time', '??:??')
                    d = evt.get('description', '') or evt.get('event', '')
                    print(f"‚öΩ [{t}] {d}")

                full_report.extend(analysis["events"])

        # 5. –û—á–∏—Å—Ç–∫–∞
        shutil.rmtree(self.output_dir, ignore_errors=True)
        return full_report